# ğŸ§  ResNets from Scratch (PyTorch)

This repository contains an **implementation of ResNet architectures (18, 34, 50)** built **from scratch in PyTorch**, without using `torchvision.models`.  
Each ResNet variant (ResNet-18, ResNet-34, and ResNet-50) is implemented using **modular building blocks**, such as basic and bottleneck residual blocks, to showcase a deep understanding of **Residual Learning**.

---

## ğŸš€ Features
- âœ… Implemented **BasicBlock** and **BottleneckBlock** manually  
- âœ… Supports **ResNet-18**, **ResNet-34**, and **ResNet-50**  
- âœ… Uses **Batch Normalization**, **Downsampling**, and **Skip Connections**  
- âœ… Fully **modular design** â€” easy to extend for other variants  
- âœ… Compatible with **CUDA**  
- âœ… Includes a **sanity check function** to verify architecture flow  

---

## ğŸ§© Architecture Overview

| Model      | Blocks per Layer | Type         | Bottleneck | Params (Approx) |
|-------------|------------------|---------------|--------------|------------------|
| ResNet-18   | [2, 2, 2, 2]     | BasicBlock    | âŒ           | ~11M             |
| ResNet-34   | [3, 4, 6, 3]     | BasicBlock    | âŒ           | ~21M             |
| ResNet-50   | [3, 4, 6, 3]     | Bottleneck    | âœ…           | ~25M             |

---

## ğŸ§± Components
- **Block** â†’ Implements 2 convolutional layers with skip connection.  
- **BottleneckBlock** â†’ Implements 1Ã—1 â†’ 3Ã—3 â†’ 1Ã—1 architecture used in deeper networks.  
- **Layer** â†’ Stack of multiple blocks, handles downsampling for the first block.  
- **ResNet** â†’ Final architecture combining all layers, pooling, and fully connected head.

---

## ğŸ§‘â€ğŸ’» Author

Jaskeerat Singh
ğŸ¯ AI/ML Engineer | Passionate about Deep Learning Architectures